{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RedditSentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxTvh/oAJVlYWLhKQ4CusU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cullena20/RedditSentimentWebsite/blob/main/RedditSentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b457c2QEIf-6"
      },
      "source": [
        "# Reddit Sentiment Analysis!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4Y4aLBB8EGk",
        "outputId": "12af686f-45eb-4282-e4ad-a1cf604b8fe6"
      },
      "source": [
        "from IPython import display  # control displaying of printed output in loops\n",
        "from pprint import pprint  # pretty print json and lists\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "!pip install praw\n",
        "import praw"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/15/4bcc44271afce0316c73cd2ed35f951f1363a07d4d5d5440ae5eb2baad78/praw-7.1.0-py3-none-any.whl (152kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 15.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 51kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 71kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 81kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 92kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 4.8MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.17\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/ba/8dd7fa5f0b1c6a8ac62f8f57f7e794160c1f86f31c6d0fb00f582372a3e4/update_checker-0.18.0-py3-none-any.whl\n",
            "Collecting prawcore<2.0,>=1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/40/b741437ce4c7b64f928513817b29c0a615efb66ab5e5e01f66fe92d2d95b/prawcore-1.5.0-py3-none-any.whl\n",
            "Collecting websocket-client>=0.54.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update-checker>=0.17->praw) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (2.10)\n",
            "Installing collected packages: update-checker, prawcore, websocket-client, praw\n",
            "Successfully installed praw-7.1.0 prawcore-1.5.0 update-checker-0.18.0 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1hZpf2i8Zwm",
        "outputId": "f751cdac-bfda-4627-bb5d-36c7acd00b90"
      },
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzeGsbTrIkI2"
      },
      "source": [
        "## Exploring the Reddit API using PRAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD11pM1p-M1E"
      },
      "source": [
        "Access the Reddit API. This allows you to easilly acess data from Reddit. To do this go to https://www.reddit.com/prefs/apps/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCrvjelK8_Kf"
      },
      "source": [
        "reddit = praw.Reddit(client_id='<your client_id>',\n",
        "                     client_secret='<your client_secret>',\n",
        "                     user_agent='<your user_agent>',\n",
        "                     username='<your user_name')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnC0rmmcFQBU",
        "outputId": "4b9e98f9-5797-4fed-9210-7e5b57d03a93"
      },
      "source": [
        "# subreddit1 = reddit.subreddits.search_by_name('datascience', exact=True)  returns a list of search results\n",
        "subreddit_name = 'datascience'\n",
        "subreddit = reddit.subreddit(subreddit_name)\n",
        "print(\"Display Name:\")\n",
        "print(subreddit.display_name) \n",
        "print()\n",
        "print(\"Title:\")\n",
        "print(subreddit.title)   \n",
        "print()\n",
        "print(\"Description\")      \n",
        "print(subreddit.description) "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Display Name:\n",
            "datascience\n",
            "\n",
            "Title:\n",
            "Data Science\n",
            "\n",
            "Description\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bJJQ4n4-Skx",
        "outputId": "91ec418a-0d60-4d9b-bf5b-c2d01f915824"
      },
      "source": [
        "posts = set()  # use a set to clear any duplicates\n",
        "for post in subreddit.new(limit=None):\n",
        "  posts.add(post)\n",
        "  display.clear_output()  # only one output that changes\n",
        "  print(len(posts))\n",
        "posts = list(posts)  # easier to work with lists"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "730\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8ScY3aqCJ2Y",
        "outputId": "66e6c919-e00d-4135-e7d0-19151364477b"
      },
      "source": [
        "post = posts[2]\n",
        "print(post.title)\n",
        "print(post.author)\n",
        "print(post.score)\n",
        "print(post.id)\n",
        "print(post.url)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I'm bored of hearing about ML and about the DS career path.\n",
            "guinea_fowler\n",
            "52\n",
            "jwa241\n",
            "https://www.reddit.com/r/datascience/comments/jwa241/im_bored_of_hearing_about_ml_and_about_the_ds/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SskqNtQmdrzH"
      },
      "source": [
        "Using the Reddit API, we can also explore comments. Maybe we can make a model that looks at comments as well as titles?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7RN6nTtdzd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462eb32f-5b18-450d-b1fc-f5fbe8a21dd9"
      },
      "source": [
        "# this creates a list of comments from the post we already defined\n",
        "comments = list(post.comments)\n",
        "# pprint(vars(comments[1]))  # gives us variables for comment\n",
        "print('Post Title:', post.title)\n",
        "print()\n",
        "print('Comment: ', comments[1].body)\n",
        "print()\n",
        "print('Comment Author: ', comments[1].author)\n",
        "print('Score: ', comments[1].score)  # would be nice to have model weigh this too"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Post Title: I'm bored of hearing about ML and about the DS career path.\n",
            "\n",
            "Comment:  I'm currently doing causal analysis to determine the root drivers of sexual violence in a particular developing country. So identifying differing factors such as education, wealth status, assets, density of population in living area etc. Have stratified sample data to represent the entire population of the country. \n",
            "\n",
            "fyi Social Science work\n",
            "\n",
            "Comment Author:  stingFC\n",
            "Score:  17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsXhLQnQIoz0"
      },
      "source": [
        "## Basic Sentiment Analysis Using Pretrained Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMG0DhInVLz9"
      },
      "source": [
        "For now, we will explore various pretrained models that detect negative and positive sentiment. Alternativley, we could train our own model using a dataset and sklearn. However, these pretrained models actually perform pretty well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7obsCCBQb9oZ"
      },
      "source": [
        "The main model that we are using is vader from nltk. This model has been pretrained specifically for social media text. A detailed paper describing the model can be found at https://www.researchgate.net/publication/275828927_VADER_A_Parsimonious_Rule-based_Model_for_Sentiment_Analysis_of_Social_Media_Text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9QabdFhLaxn"
      },
      "source": [
        "Code for other models is commented out because we are not using them. You can uncomment to explore them though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPV6VqnJTQoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39baf2a9-7197-428b-8439-47af7a7dcf77"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "sia = SIA()\n",
        "# from textblob import TextBlob\n",
        "# !pip install flair\n",
        "# import flair\n",
        "# flair_sentiment = flair.models.TextClassifier.load('en-sentiment')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fx_Qjw8TTcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506a72e7-42ae-424f-946a-5ecdc6d07e64"
      },
      "source": [
        "sentence = \"This food was great but the service was only okay\"\n",
        "print(\"NLTK VADER\")\n",
        "print(sia.polarity_scores(sentence))\n",
        "# print()\n",
        "# print(\"Text Blob:\")\n",
        "# print(TextBlob(sentence).sentiment)\n",
        "# print()\n",
        "# print(\"Flair:\")\n",
        "# s = flair.data.Sentence(sentence)\n",
        "# flair_sentiment.predict(s)\n",
        "# total_sentiment = s.labels\n",
        "# print(total_sentiment)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK VADER\n",
            "{'neg': 0.0, 'neu': 0.62, 'pos': 0.38, 'compound': 0.5994}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eslGmTm-7By",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d02b9087-fe9f-49b2-d319-04cb0ddb05d4"
      },
      "source": [
        "results = list()\n",
        "\n",
        "for post in posts:\n",
        "    pol_score = sia.polarity_scores(post.title)\n",
        "    pol_score['headline'] = post.title\n",
        "    results.append(pol_score)\n",
        "\n",
        "pprint(results[:3], width=100)  # pretty print"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'compound': -0.5574,\n",
            "  'headline': 'Seriously, how am I expected to grow in a profession where everyone discourages me '\n",
            "              'from building anything non-trivial',\n",
            "  'neg': 0.247,\n",
            "  'neu': 0.753,\n",
            "  'pos': 0.0},\n",
            " {'compound': 0.0,\n",
            "  'headline': 'DS professional looking for guidance to build out NLP skillset',\n",
            "  'neg': 0.0,\n",
            "  'neu': 1.0,\n",
            "  'pos': 0.0},\n",
            " {'compound': -0.2732,\n",
            "  'headline': \"I'm bored of hearing about ML and about the DS career path.\",\n",
            "  'neg': 0.16,\n",
            "  'neu': 0.84,\n",
            "  'pos': 0.0}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZybOo6M-LoB0"
      },
      "source": [
        "Now we will store the data as a pandas dataframe. We will create a new column, 'label', that will store if the headline is positive (1), neutral (0), or negative (-1). We have used 0.2 and -0.2 as our thresholds but this can be altered (giving us different results)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4A_SvUX_78G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "49d81c6f-ca16-46db-f0a8-df5c3c70a716"
      },
      "source": [
        "df = pd.DataFrame.from_records(results)\n",
        "sorted_df = df.sort_values(by='compound')\n",
        "df['label'] = 0  # creates label column\n",
        "df.loc[df['compound'] > 0.2, 'label'] = 1  # if compound score is greater than 0.2 we label it as positive\n",
        "df.loc[df['compound'] < -0.2, 'label'] = -1  # if compound score is less than -0.2 we label it as positive\n",
        "df.sample(n=10,axis='rows')  # prints 10 random items from the dataframe"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>compound</th>\n",
              "      <th>headline</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>366</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Does anyone here use orange 3?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.784</td>\n",
              "      <td>0.216</td>\n",
              "      <td>0.0258</td>\n",
              "      <td>Do the course certificates matter?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>714</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.940</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.4019</td>\n",
              "      <td>I am a junior data analyst with an Econ backgr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>What is your take on job postings that ask for...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Using Decision Trees to Find Business Rules - ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.903</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.4404</td>\n",
              "      <td>I'm in the market for a laptop. As a data anal...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.4588</td>\n",
              "      <td>Do people have luck using \"modern\" resume temp...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>530</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Non-Negotiable salary Offer for a Lead Analyst...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.649</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0.4019</td>\n",
              "      <td>Data science relate christmas holiday puns</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>705</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Data science career path?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     neg    neu  ...                                           headline  label\n",
              "366  0.0  1.000  ...                     Does anyone here use orange 3?      0\n",
              "48   0.0  0.784  ...                 Do the course certificates matter?      0\n",
              "714  0.0  0.940  ...  I am a junior data analyst with an Econ backgr...      1\n",
              "377  0.0  1.000  ...  What is your take on job postings that ask for...      0\n",
              "309  0.0  1.000  ...  Using Decision Trees to Find Business Rules - ...      0\n",
              "267  0.0  0.903  ...  I'm in the market for a laptop. As a data anal...      1\n",
              "305  0.0  0.700  ...  Do people have luck using \"modern\" resume temp...      1\n",
              "530  0.0  1.000  ...  Non-Negotiable salary Offer for a Lead Analyst...      0\n",
              "510  0.0  0.649  ...         Data science relate christmas holiday puns      1\n",
              "705  0.0  1.000  ...                          Data science career path?      0\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwigf8cYIyeQ"
      },
      "source": [
        "Now that we have our results, we can save them in a csv file!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKpGjehm-rDW"
      },
      "source": [
        "df.to_csv('{}_sentiment'.format(subreddit_name))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDe78KTW-ykQ"
      },
      "source": [
        "The below code clearly shows the analysis of a subreddit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HljLKTcKHxZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b6b206-735b-4fd9-a694-57b86948c876"
      },
      "source": [
        "percentages = df.label.value_counts(normalize=True) * 100\n",
        "for key in percentages.keys():\n",
        "    if key == -1:\n",
        "        percentages['Negative'] = percentages[key]\n",
        "        del percentages[key]\n",
        "    if key == 0:\n",
        "        percentages['Neutral'] = percentages[key]\n",
        "        del percentages[key]\n",
        "    if key == 1:\n",
        "        percentages['Positive'] = percentages[key]\n",
        "        del percentages[key]\n",
        "print(percentages)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neutral     61.095890\n",
            "Positive    28.082192\n",
            "Negative    10.821918\n",
            "Name: label, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awbBITlWI4B_"
      },
      "source": [
        "## Exploring Our Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6Y4XEgLMDLj"
      },
      "source": [
        "We can explore the most positive and negative headlines using the below code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xIALNGr7SM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7eedcb-6d83-4d63-fb1c-754d3f57c154"
      },
      "source": [
        "sorted_df = df.sort_values(by='compound')\n",
        "print('Five Most Positive Titles:')\n",
        "for headline in list(sorted_df.tail(5)['headline']):\n",
        "  print(headline)\n",
        "print()\n",
        "print('Five Most Negative Titles:')\n",
        "for headline in list(sorted_df.head(5)['headline']):\n",
        "  print(headline)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Five Most Positive Titles:\n",
            "Best resource to learn Excel for data science?\n",
            "As a data scientist, what action should be taken to have the greatest positive impact on society and the environment?\n",
            "Why do you love / hate about the Data Science field ? And how it compare to Software Engineering. I'm very appreciated about your sharing. Thank you very much.\n",
            "After spending more than a year as a data scientist I found these 4 hard truths data science blogs don't teach you about. I hope sharing my journey helps you in some way.\n",
            "A whole year from now to start applying to DS jobs. I'm in a good situation. I can choose what skills to improve, and I'm sure your advice will be super helpful.\n",
            "\n",
            "Five Most Negative Titles:\n",
            "What was your most WTF analysis or insight obtained?\n",
            "Angry rant\n",
            "Did anyone regret choosing DS as a career or has got disillusioned with it?\n",
            "Unusual question but: does your data science experience make you more aware of congnitive traps that some people fall into when believing conspiracy theories?\n",
            "What is the difference between training error and in-sample error ? (Elements of Statistical Learning)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfNPwecTMr5H"
      },
      "source": [
        "This code will print the first five negative results and the first five positive results. These do not take into account how positive or negative that they are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcXJMJu_JaYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "341b9bb1-08cb-4160-f8b8-5e2a54c2a17e"
      },
      "source": [
        "positive_results = df[df['label'] == 1]\n",
        "negative_results = df[df['label'] == -1]\n",
        "print(\"Postitive Results:\")\n",
        "pprint(list(positive_results['headline'])[:5]) \n",
        "print()\n",
        "print(\"Negative Results:\")\n",
        "pprint(list(negative_results['headline'])[:5]) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Postitive Results:\n",
            "['Best DS setup within company firewall/proxy',\n",
            " 'Dear Data scientists on this forum at Twitter & Facebook - please do '\n",
            " 'something at your companies',\n",
            " 'Would anyone be interested in a “soft data science” series?',\n",
            " 'How to go about creating models without labels',\n",
            " 'Data Engineer, watching upper management contract out work that I feel that '\n",
            " \"I'm capable of doing\"]\n",
            "\n",
            "Negative Results:\n",
            "['Seriously, how am I expected to grow in a profession where everyone '\n",
            " 'discourages me from building anything non-trivial',\n",
            " \"I'm bored of hearing about ML and about the DS career path.\",\n",
            " 'How do we stop AI or machine learning algorithms from being “racist sexist '\n",
            " 'xenophobic” etc',\n",
            " 'Is putting data camp certificates on linkedin obnoxious?',\n",
            " 'Averaging margin of errors of polls']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paV_vrAvdfvq"
      },
      "source": [
        "Now we can determine the overall sentiment of a subreddit by creating percentages of positive, neutral, and negative headlines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tuel0lDlKUnh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "687bbf29-c67d-483b-ec9b-cdcf63c5507d"
      },
      "source": [
        "percentages = df.label.value_counts(normalize=True) * 100\n",
        "print(\"Count:\")\n",
        "print(df.label.value_counts())\n",
        "print()\n",
        "print(\"Percentages:\")\n",
        "print(percentages)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count:\n",
            " 0    446\n",
            " 1    205\n",
            "-1     79\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Percentages:\n",
            " 0    61.095890\n",
            " 1    28.082192\n",
            "-1    10.821918\n",
            "Name: label, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCH-ngEsdmbE"
      },
      "source": [
        "The below code lets us visualize the above results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H74CRUr-Ktpy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "4662bbb0-46a5-4255-c011-09c46c807589"
      },
      "source": [
        "sns.barplot(x=percentages.index, y=percentages)\n",
        "plt.xlabel = ['Negative', 'Nuetral', 'Positive']\n",
        "plt.plot()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbNe5RYMeVQZ"
      },
      "source": [
        "## Future Model Improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue6mdh9zeawh"
      },
      "source": [
        "We classified a subreddit's sentiment soley based on the titles of its top posts. Maybe we can look at more data such as comments and upvotes. Using this same idea, we may also be able to make a more practical model, such as a fake news or hate speech detector. This would involve using different models and potentially needing training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-XSijy2ixGW"
      },
      "source": [
        "## Now What?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNPJcZUDd5R4"
      },
      "source": [
        "In this notebook, we have explored the Reddit API, and different sentiment analysis models. We have also been able to visualize our results. Using these ideas and the code as a foundation, we can turn this into something more accesible. For example, we can make a website where a user types in a subreddit and get a sentiment analysis back. We can make a cli to\n",
        " analyze subreddits in the command line."
      ]
    }
  ]
}