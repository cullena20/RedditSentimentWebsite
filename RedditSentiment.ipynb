{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RedditSentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hx2MOqQmaSGV"
      ],
      "authorship_tag": "ABX9TyMNiTDg7MHrpsRAAkHq0YWx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cullena20/RedditSentimentWebsite/blob/main/RedditSentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b457c2QEIf-6"
      },
      "source": [
        "# Reddit Sentiment Analysis!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4Y4aLBB8EGk",
        "outputId": "acb7c4c8-81f1-41a8-d463-de0883b7f8d1"
      },
      "source": [
        "from IPython import display  # control displaying of printed output in loops\n",
        "from pprint import pprint  # pretty print json and lists\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "!pip install praw\n",
        "import praw"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/15/4bcc44271afce0316c73cd2ed35f951f1363a07d4d5d5440ae5eb2baad78/praw-7.1.0-py3-none-any.whl (152kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 4.3MB/s \n",
            "\u001b[?25hCollecting prawcore<2.0,>=1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/40/b741437ce4c7b64f928513817b29c0a615efb66ab5e5e01f66fe92d2d95b/prawcore-1.5.0-py3-none-any.whl\n",
            "Collecting websocket-client>=0.54.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 29.5MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.17\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/ba/8dd7fa5f0b1c6a8ac62f8f57f7e794160c1f86f31c6d0fb00f582372a3e4/update_checker-0.18.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from prawcore<2.0,>=1.3.0->praw) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.3.0->praw) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.3.0->praw) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.3.0->praw) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.3.0->praw) (1.24.3)\n",
            "Installing collected packages: prawcore, websocket-client, update-checker, praw\n",
            "Successfully installed praw-7.1.0 prawcore-1.5.0 update-checker-0.18.0 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1hZpf2i8Zwm",
        "outputId": "41a3a7f4-fa5d-401b-d6ea-6d87e059d192"
      },
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzeGsbTrIkI2"
      },
      "source": [
        "## Exploring the Reddit API using PRAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD11pM1p-M1E"
      },
      "source": [
        "Access the Reddit API. This allows you to easilly acess data from Reddit. To do this go to https://www.reddit.com/prefs/apps/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCrvjelK8_Kf"
      },
      "source": [
        "reddit = praw.Reddit(client_id='<your client_id>',\n",
        "                     client_secret='<your client_secret>',\n",
        "                     user_agent='<your user_agent>',\n",
        "                     username='<your user_name')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnC0rmmcFQBU",
        "outputId": "9e9a7472-e263-4b76-c82f-31ec647416cf"
      },
      "source": [
        "# subreddit1 = reddit.subreddits.search_by_name('datascience', exact=True)  returns a list of search results\n",
        "subreddit = reddit.subreddit('datascience')\n",
        "print(\"Display Name:\")\n",
        "print(subreddit.display_name) \n",
        "print()\n",
        "print(\"Title:\")\n",
        "print(subreddit.title)   \n",
        "print()\n",
        "print(\"Description\")      \n",
        "print(subreddit.description) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Display Name:\n",
            "datascience\n",
            "\n",
            "Title:\n",
            "Data Science\n",
            "\n",
            "Description\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bJJQ4n4-Skx",
        "outputId": "786023a0-6707-40ff-e254-975dcc56524a"
      },
      "source": [
        "posts = set()  # use a set to clear any duplicates\n",
        "for post in subreddit.new(limit=None):\n",
        "  posts.add(post)\n",
        "  display.clear_output()  # only one output that changes\n",
        "  print(len(posts))\n",
        "posts = list(posts)  # easier to work with lists"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8ScY3aqCJ2Y",
        "outputId": "3d9d556c-c544-4308-840e-70034eec3090"
      },
      "source": [
        "post = posts[2]\n",
        "print(post.title)\n",
        "print(post.author)\n",
        "print(post.score)\n",
        "print(post.id)\n",
        "print(post.url)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Are there any people who started off with data science with a non-computer science background after they started working but still managed to make a decent career in it?\n",
            "None\n",
            "280\n",
            "k5y56t\n",
            "https://www.reddit.com/r/datascience/comments/k5y56t/are_there_any_people_who_started_off_with_data/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SskqNtQmdrzH"
      },
      "source": [
        "Using the Reddit API, we can also explore comments. Maybe we can make a model that looks at comments as well as titles?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7RN6nTtdzd9"
      },
      "source": [
        "# this creates a list of comments from the post we already defined\n",
        "comments = list(post.comments)\n",
        "# pprint(vars(comments[1]))  # gives us variables for comment\n",
        "print('Post Title:', post.title)\n",
        "print()\n",
        "print('Comment: ', comments[1].body)\n",
        "print()\n",
        "print('Comment Author: ', comments[1].author)\n",
        "print('Score: ', comments[1].score)  # would be nice to have model weigh this too"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsXhLQnQIoz0"
      },
      "source": [
        "## Basic Sentiment Analysis Using Pretrained Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMG0DhInVLz9"
      },
      "source": [
        "For now, we will explore various pretrained models that detect negative and positive sentiment. Alternativley, we could train our own model using a dataset and sklearn. However, these pretrained models actually perform pretty well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7obsCCBQb9oZ"
      },
      "source": [
        "The main model that we are using is vader from nltk. This model has been pretrained specifically for social media text. A detailed paper describing the model can be found at https://www.researchgate.net/publication/275828927_VADER_A_Parsimonious_Rule-based_Model_for_Sentiment_Analysis_of_Social_Media_Text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9QabdFhLaxn"
      },
      "source": [
        "Code for other models is commented out because we are not using them. You can uncomment to explore them though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPV6VqnJTQoS"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "sia = SIA()\n",
        "# from textblob import TextBlob\n",
        "# !pip install flair\n",
        "# import flair\n",
        "# flair_sentiment = flair.models.TextClassifier.load('en-sentiment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fx_Qjw8TTcA"
      },
      "source": [
        "sentence = \"This food was great but the service was only okay\"\n",
        "print(\"NLTK VADER\")\n",
        "print(sia.polarity_scores(sentence))\n",
        "# print()\n",
        "# print(\"Text Blob:\")\n",
        "# print(TextBlob(sentence).sentiment)\n",
        "# print()\n",
        "# print(\"Flair:\")\n",
        "# s = flair.data.Sentence(sentence)\n",
        "# flair_sentiment.predict(s)\n",
        "# total_sentiment = s.labels\n",
        "# print(total_sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eslGmTm-7By"
      },
      "source": [
        "results = list()\n",
        "\n",
        "for post in posts:\n",
        "    pol_score = sia.polarity_scores(post.title)\n",
        "    pol_score['headline'] = post.title\n",
        "    results.append(pol_score)\n",
        "\n",
        "pprint(results[:3], width=100)  # pretty print"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZybOo6M-LoB0"
      },
      "source": [
        "Now we will store the data as a pandas dataframe. We will create a new column, 'label', that will store if the headline is positive (1), neutral (0), or negative (-1). We have used 0.2 and -0.2 as our thresholds but this can be altered (giving us different results)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4A_SvUX_78G"
      },
      "source": [
        "df = pd.DataFrame.from_records(results)\n",
        "sorted_df = df.sort_values(by='compound')\n",
        "df['label'] = 0  # creates label column\n",
        "df.loc[df['compound'] > 0.2, 'label'] = 1  # if compound score is greater than 0.2 we label it as positive\n",
        "df.loc[df['compound'] < -0.2, 'label'] = -1  # if compound score is less than -0.2 we label it as positive\n",
        "df.sample(n=10,axis='rows')  # prints 10 random items from the dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwigf8cYIyeQ"
      },
      "source": [
        "Now that we have our results, we can save them in a csv file!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HljLKTcKHxZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e3552fa-21ab-494f-97b4-3d9182f2e660"
      },
      "source": [
        "percentages = dict(df.label.value_counts(normalize=True) * 100)\n",
        "for key in percentages.keys():\n",
        "    if key == -1:\n",
        "        percentages['Negative'] = percentages[key]\n",
        "        del percentages[key]\n",
        "    if key == 0:\n",
        "        percentages['Neutral'] = percentages[key]\n",
        "        del percentages[key]\n",
        "    if key == 1:\n",
        "        percentages['Positive'] = percentages[key]\n",
        "        del percentages[key]\n",
        "percentages"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Negative': 9.581646423751687,\n",
              " 'Neutral': 62.61808367071525,\n",
              " 'Positive': 27.800269905533064}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awbBITlWI4B_"
      },
      "source": [
        "## Exploring Our Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6Y4XEgLMDLj"
      },
      "source": [
        "We can explore the most positive and negative headlines using the below code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xIALNGr7SM1"
      },
      "source": [
        "sorted_df = df.sort_values(by='compound')\n",
        "print('Five Most Positive Titles:')\n",
        "for headline in list(sorted_df.tail(5)['headline']):\n",
        "  print(headline)\n",
        "print()\n",
        "print('Five Most Negative Titles:')\n",
        "for headline in list(sorted_df.head(5)['headline']):\n",
        "  print(headline)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfNPwecTMr5H"
      },
      "source": [
        "This code will print the first five negative results and the first five positive results. These do not take into account how positive or negative that they are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcXJMJu_JaYf"
      },
      "source": [
        "positive_results = df[df['label'] == 1]\n",
        "negative_results = df[df['label'] == -1]\n",
        "print(\"Postitive Results:\")\n",
        "pprint(list(positive_results['headline'])[:5]) \n",
        "print()\n",
        "print(\"Negative Results:\")\n",
        "pprint(list(negative_results['headline'])[:5]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paV_vrAvdfvq"
      },
      "source": [
        "Now we can determine the overall sentiment of a subreddit by creating percentages of positive, neutral, and negative headlines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tuel0lDlKUnh",
        "outputId": "a2b0c359-f417-45d1-88ea-418cf4ccae17"
      },
      "source": [
        "percentages = df.label.value_counts(normalize=True) * 100\n",
        "print(\"Count:\")\n",
        "print(df.label.value_counts())\n",
        "print()\n",
        "print(\"Percentages:\")\n",
        "print(percentages)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count:\n",
            " 0    464\n",
            " 1    206\n",
            "-1     71\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Percentages:\n",
            " 0    62.618084\n",
            " 1    27.800270\n",
            "-1     9.581646\n",
            "Name: label, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCH-ngEsdmbE"
      },
      "source": [
        "The below code lets us visualize the above results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H74CRUr-Ktpy"
      },
      "source": [
        "sns.barplot(x=percentages.index, y=percentages)\n",
        "plt.xlabel = ['Negative', 'Nuetral', 'Positive']\n",
        "plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbNe5RYMeVQZ"
      },
      "source": [
        "## Future Model Improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue6mdh9zeawh"
      },
      "source": [
        "We classified a subreddit's sentiment soley based on the titles of its top posts. Maybe we can look at more data such as comments and upvotes. Using this same idea, we may also be able to make a more practical model, such as a fake news or hate speech detector. This would involve using different models and potentially needing training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-XSijy2ixGW"
      },
      "source": [
        "## Now What?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNPJcZUDd5R4"
      },
      "source": [
        "In this notebook, we have explored the Reddit API, and different sentiment analysis models. We have also been able to visualize our results. Using these ideas and the code as a foundation, we can turn this into something more accesible. For example, we can make a website where a user types in a subreddit and get a sentiment analysis back. We can make a cli to do analyze subreddits in the command line."
      ]
    }
  ]
}